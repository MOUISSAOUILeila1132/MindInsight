# main.py - Version Corrig√©e avec √©v√©nements FastAPI

# --- Standard Libraries ---
import os
import time
import traceback # Pour un meilleur d√©bogage
import sys # Pour utiliser sys.exit() si n√©cessaire (bien que database.py le fasse d√©j√†)

# --- Third-Party Libraries ---
import torch
import torch.nn.functional as F
from fastapi import FastAPI, HTTPException # Importer ici une seule fois
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import BertTokenizer, BertForSequenceClassification
import tweepy
import pandas as pd
from dotenv import load_dotenv

# --- Local Application Imports ---
import database  # Importe le module database pour acc√©der √† ses fonctions
from routers import doctors, patients # Importe le routeur depuis le dossier routers

# --- Configuration & Initialisation (ex√©cut√© une seule fois au d√©marrage du script) ---
print("üöÄ D√©marrage du script principal de l'API...")
load_dotenv() # Charge les variables depuis .env

# --- Configuration IA & Twitter ---
MODEL_PATH = os.getenv("MODEL_PATH", "model/bert_mental_health_model.bin") # Utiliser getenv avec d√©faut
BEARER_TOKEN = os.getenv("TWITTER_BEARER_TOKEN")

if not BEARER_TOKEN:
    print("‚ùå ERREUR CRITIQUE: TWITTER_BEARER_TOKEN non trouv√© dans les variables d'environnement.")
    sys.exit(1) # Utiliser sys.exit(1) ici aussi pour coh√©rence

print("üê¶ Initialisation du client Twitter...")
try:
    twitter_client = tweepy.Client(
        bearer_token=BEARER_TOKEN,
        wait_on_rate_limit=True,
        # connection_timeout=10, # Optionnel: timeout de connexion
        # request_timeout=30     # Optionnel: timeout de requ√™te
    )
    # Note: Une simple initialisation ne garantit pas la connexion. Un appel test peut √™tre utile.
    # Ex: user_info = twitter_client.get_me()
    # if not user_info.data:
    #     raise Exception("Impossible de r√©cup√©rer les informations de l'utilisateur Twitter (Bearer Token invalide ?)")
    print("‚úÖ Client Twitter initialis√©.")
except tweepy.errors.TweepyException as e:
     print(f"‚ùå ERREUR CRITIQUE: √âchec de l'initialisation du client Twitter: {e}")
     sys.exit(1) # Quitter en cas d'√©chec critique
except Exception as e:
    print(f"‚ùå ERREUR CRITIQUE: Erreur inattendue lors de l'initialisation de Twitter: {e}")
    sys.exit(1) # Quitter en cas d'√©chec critique


print("üß† Chargement du tokenizer et du mod√®le BERT...")
try:
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    # Assurez-vous que NUM_LABELS est d√©fini AVANT de charger le mod√®le
    # !! Assurez-vous que CLASS_LABELS est d√©fini et correct pour votre mod√®le !!
    CLASS_LABELS = ["Normal", "Stressed", "Anxiety", "Depression", "Potential Suicide Post"] # Les 5 labels corrects
    NUM_LABELS = len(CLASS_LABELS)

    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=NUM_LABELS)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Utilisation du device: {device}")

    if not os.path.exists(MODEL_PATH):
         print(f"‚ùå ERREUR CRITIQUE: Fichier mod√®le non trouv√© √† l'emplacement: {MODEL_PATH}")
         sys.exit(1)

    # Charger les poids du mod√®le avec map_location
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device) # Mettre le mod√®le sur le bon device
    model.eval() # Mettre le mod√®le en mode √©valuation (d√©sactive dropout, etc.)
    print("‚úÖ Mod√®le BERT charg√© avec succ√®s.")

except FileNotFoundError:
     print(f"‚ùå ERREUR CRITIQUE: Fichier mod√®le non trouv√© √† l'emplacement: {MODEL_PATH}")
     sys.exit(1) # Redondant avec la v√©rification os.path.exists, mais clair
except Exception as e:
    print(f"‚ùå ERREUR CRITIQUE: Erreur lors du chargement du mod√®le/tokenizer BERT: {e}")
    traceback.print_exc()
    sys.exit(1) # Quitter en cas d'√©chec critique


# --- D√©finition des Fonctions d'aide (Analyse IA) ---
# ... (Les fonctions get_user_tweets et predict_mental_state restent identiques) ...

def get_user_tweets(username: str, max_results: int = 10):
    """R√©cup√®re les tweets d'un utilisateur Twitter."""
    print(f"\nüîç Tentative de r√©cup√©ration des tweets pour @{username} (max: {max_results})....")
    try:
        # 1. Obtenir l'ID de l'utilisateur
        print(f"   Obtention de l'ID utilisateur pour @{username}...")
        user_response = twitter_client.get_user(username=username)
        if not user_response.data:
            print(f"   ‚ö†Ô∏è Utilisateur Twitter @{username} non trouv√©.")
            raise HTTPException(status_code=404, detail=f"L'utilisateur Twitter @{username} n'a pas √©t√© trouv√©.")
        user_id = user_response.data.id
        print(f"   ID Utilisateur: {user_id}")

        # 2. R√©cup√©rer les tweets
        print(f"   R√©cup√©ration des {max_results} derniers tweets...")
        fetch_count = max(5, min(100, max_results)) # Assurer que c'est dans les limites de l'API

        tweets_response = twitter_client.get_users_tweets(
            id=user_id,
            max_results=fetch_count,
            tweet_fields=["created_at", "public_metrics"] # Champs n√©cessaires
        )

        if not tweets_response.data:
            print(f"   ‚ÑπÔ∏è Aucun tweet r√©cent trouv√© pour @{username}.")
            return [] # Retourner une liste vide est coh√©rent

        # 3. Formater les tweets
        formatted_tweets = []
        for tweet in tweets_response.data:
            likes = tweet.public_metrics.get("like_count", 0) if tweet.public_metrics else 0
            retweets = tweet.public_metrics.get("retweet_count", 0) if tweet.public_metrics else 0
            formatted_tweets.append({
                "id": tweet.id,
                "text": tweet.text,
                "created_at": str(tweet.created_at), # Convertir en string pour JSON
                "likes": likes,
                "retweets": retweets
            })

        print(f"‚úÖ R√©cup√©r√© {len(formatted_tweets)} tweets pour @{username}.")
        return formatted_tweets

    except tweepy.errors.NotFound:
         print(f"   ‚ö†Ô∏è Utilisateur Twitter @{username} non trouv√© (Tweepy NotFound).")
         raise HTTPException(status_code=404, detail=f"L'utilisateur Twitter @{username} n'a pas √©t√© trouv√©.")
    except tweepy.errors.TweepyException as e:
        print(f"   ‚ùå Erreur API Twitter pour @{username}: {e}")
        error_detail = f"Erreur de l'API Twitter: {e}"
        try:
             if hasattr(e, 'api_codes') and e.api_codes and hasattr(e, 'api_errors') and e.api_errors:
                  error_detail = f"Erreur API Twitter {e.api_codes[0]}: {e.api_errors[0]}"
        except Exception:
            pass
        raise HTTPException(status_code=503, detail=error_detail)
    except Exception as e:
        print(f"   ‚ùå Erreur inattendue lors de la r√©cup√©ration des tweets pour @{username}: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="Erreur interne inattendue lors de la r√©cup√©ration des tweets.")


def predict_mental_state(text: str):
    """Pr√©dit l'√©tat mental √† partir d'un texte en utilisant le mod√®le BERT."""
    if not text or not isinstance(text, str):
        print("   ‚ö†Ô∏è Texte invalide fourni pour la pr√©diction.")
        return "Invalid Input", {label: 0.0 for label in CLASS_LABELS}

    try:
        inputs = tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(device)

        with torch.no_grad():
            outputs = model(**inputs)

        logits = outputs.logits
        probabilities = F.softmax(logits, dim=1)[0]
        predicted_index = torch.argmax(probabilities).item()
        predicted_label = CLASS_LABELS[predicted_index]

        probabilities_dict = {
            CLASS_LABELS[i]: round(probabilities[i].item() * 100, 2)
            for i in range(len(CLASS_LABELS))
        }

        return predicted_label, probabilities_dict

    except Exception as e:
        print(f"   ‚ùå Erreur pendant la pr√©diction BERT pour le texte '{text[:50]}...': {e}")
        traceback.print_exc()
        return "Prediction Error", {label: 0.0 for label in CLASS_LABELS}


# --- FastAPI Application Setup ---
app = FastAPI(
    title="API Analyse IA & Gestion Docteurs",
    description="Combine l'analyse IA de profils Twitter et l'inscription/gestion des docteurs.",
    version="1.1.0"
)

# --- Handlers d'√âv√©nements de D√©marrage et d'Arr√™t ---
# !! C'est l'ajout cl√© pour l'initialisation de la base de donn√©es !!

@app.on_event("startup")
def startup_event():
    """
    Fonction ex√©cut√©e par FastAPI au d√©marrage de l'application.
    Initialise la connexion √† la base de donn√©es et d'autres ressources si n√©cessaire.
    """
    print("üöÄ √âv√©nement de d√©marrage d√©clench√© par FastAPI.")
    # Appelle la fonction de connexion d√©finie dans database.py
    # Si la connexion √©choue, database.connect_to_mongo() appelle sys.exit(1)
    # et l'application s'arr√™te.
    database.connect_to_mongo()
    # Ici vous pourriez ajouter d'autres initialisations si besoin (ex: chargement de caches, etc.)
    print("‚úÖ √âv√©nement de d√©marrage termin√©.")


@app.on_event("shutdown")
def shutdown_event():
    """
    Fonction ex√©cut√©e par FastAPI √† l'arr√™t de l'application.
    Ferme la connexion √† la base de donn√©es et lib√®re d'autres ressources si n√©cessaire.
    """
    print("üëã √âv√©nement d'arr√™t d√©clench√© par FastAPI.")
    # Appelle la fonction de fermeture d√©finie dans database.py
    database.close_mongo_connection()
    # Ici vous pourriez ajouter d'autres nettoyages si besoin
    print("‚úÖ √âv√©nement d'arr√™t termin√©.")


# --- CORS Middleware Configuration ---
# IMPORTANT: Ajustez les origines selon l'URL exacte de votre frontend React
origins = [
    "http://localhost:8080",  # Si votre React tourne sur le port 8080
    "http://127.0.0.1:8080",
    "http://localhost:3000", # Ajoutez ceci si votre React tourne sur le port 3000 par d√©faut
    "http://127.0.0.1:3000",
    # Ajoutez l'URL de votre frontend d√©ploy√© ici en production
]

print(f"üîß Configuration CORS - Origines Autoris√©es: {origins}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,       # Liste des origines autoris√©es
    allow_credentials=True,      # Permet les cookies/authentification si n√©cessaire
    allow_methods=["*"],         # Autorise toutes les m√©thodes (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],         # Autorise tous les en-t√™tes (ex: Content-Type, Authorization)
)

# --- Inclure les Routeurs ---
# Ajoute toutes les routes d√©finies dans routers/doctors.py (pr√©fix√©es par /api)
# Ces routes vont utiliser la d√©pendance get_db() qui s'attend √† ce que db_handler.db soit initialis√©
app.include_router(doctors.router, prefix="/api") # Ajout explicite du pr√©fixe /api pour clart√©
print("‚úÖ Routeur pour les docteurs (/api/...) inclus.")

app.include_router(patients.router, prefix="/api") # <--- GARDEZ CETTE LIGNE
print("‚úÖ Routeur pour les patients (/api/patients/...) inclus.")


# --- Mod√®les Pydantic pour la route /analyze ---
class AnalyzeRequest(BaseModel):
    username: str = Field(..., description="Nom d'utilisateur Twitter √† analyser (sans le @)")
    max_tweets: int = Field(10, gt=0, le=100, description="Nombre max de tweets √† analyser (entre 1 et 100)")

class TweetPrediction(BaseModel):
     id: int
     text: str
     created_at: str
     likes: int
     retweets: int
     predicted_state: str
     probabilities: dict[str, float]

class AnalysisResult(BaseModel):
    username: str
    tweets_analyzed: int
    overall_summary: dict[str, float]
    predictions: list[TweetPrediction]


# --- Routes principales d√©finies dans main.py ---

@app.get("/", summary="V√©rification de l'√©tat de l'API", tags=["G√©n√©ral"])
async def root():
    """Endpoint simple pour confirmer que l'API est en ligne et fonctionnelle."""
    return {"message": "API Analyse IA & Gestion Docteurs est en ligne"}

@app.post("/analyze",
          response_model=AnalysisResult,
          summary="Analyser les tweets d'un utilisateur",
          tags=["Analyse IA"])
async def analyze_profile(request: AnalyzeRequest):
    """
    R√©cup√®re les tweets d'un utilisateur Twitter, pr√©dit l'√©tat mental
    pour chaque tweet et retourne un r√©sum√© global.
    """
    print(f"‚ö° Requ√™te re√ßue pour analyser @{request.username} (max_tweets: {request.max_tweets})")

    user_tweets = get_user_tweets(request.username, max_results=request.max_tweets)

    if not user_tweets:
        print(f"   ‚ÑπÔ∏è Aucun tweet analysable trouv√© pour @{request.username}.")
        return AnalysisResult(
             username=request.username,
             tweets_analyzed=0,
             overall_summary={label: 0.0 for label in CLASS_LABELS},
             predictions=[]
         )

    results = []
    total_predictions = {label: 0 for label in CLASS_LABELS}
    num_valid_predictions = 0

    print(f"   ü§ñ Analyse des {len(user_tweets)} tweets r√©cup√©r√©s...")
    for i, tweet_data in enumerate(user_tweets):
        print(f"      Tweet {i+1}/{len(user_tweets)}: '{tweet_data['text'][:60]}...'")
        predicted_label, probabilities = predict_mental_state(tweet_data['text'])

        if predicted_label not in ["Invalid Input", "Prediction Error"]:
            total_predictions[predicted_label] += 1
            num_valid_predictions += 1

        results.append(TweetPrediction(
             id=tweet_data['id'],
             text=tweet_data['text'],
             created_at=tweet_data['created_at'],
             likes=tweet_data['likes'],
             retweets=tweet_data['retweets'],
             predicted_state=predicted_label,
             probabilities=probabilities
        ))

    overall_summary_percent = {}
    if num_valid_predictions > 0:
        for label, count in total_predictions.items():
            percentage = round((count / num_valid_predictions) * 100, 1)
            overall_summary_percent[label] = percentage
        print(f"   üìä R√©sum√© calcul√© sur {num_valid_predictions} pr√©dictions valides.")
    else:
         print(f"   ‚ö†Ô∏è Aucune pr√©diction valide n'a pu √™tre faite.")
         overall_summary_percent = {label: 0.0 for label in CLASS_LABELS}

    print(f"‚úÖ Analyse termin√©e pour @{request.username}. R√©sum√©: {overall_summary_percent}")

    return AnalysisResult(
        username=request.username,
        tweets_analyzed=len(results),
        overall_summary=overall_summary_percent,
        predictions=results
    )


# --- Point d'entr√©e pour Uvicorn (lancement via terminal) ---
# L'utilisation de cette section n'est pas recommand√©e pour un lancement standard avec 'uvicorn main:app'
# car elle pourrait bypasser certains setups d'environnement ou de rechargement automatique.
# Conservez-la si vous lancez sp√©cifiquement avec 'python main.py'
# if __name__ == "__main__":
#     import uvicorn
#     print("üöÄ Lancement du serveur Uvicorn depuis main.py (pour d√©veloppement)...")
#     # Note: Les √©v√©nements startup/shutdown sont g√©r√©s par uvicorn lorsqu'il est lanc√© via cette m√©thode
#     uvicorn.run(
#         "main:app",
#         host="0.0.0.0",
#         port=8000,
#         reload=True,
#         log_level="info"
#     )

print("‚úÖ Ex√©cution du script principal termin√©e. Pr√™t √† √™tre d√©marr√© par Uvicorn.")
# Le message "API Analyse IA & Gestion Docteurs est en ligne" sera affich√©
# lorsque l'√©v√©nement startup sera termin√© par Uvicorn.